# LunchBadger Example Helm Installation to Minikube
#
# IMPORTANT:
# review the comments below to see if values need to be changed for your local instance
#
# notes:
# - the LunchBadger helm charts made many previous assumptions to be installed on AWS and had to be adapted to run on Minikube
#   there are bound to be kinks still
# - certain limitations like LoadBalancer references needs to be manually externalized by patching the service and or setting up
#   a bare metal loadbalancer like MetaLB or equivalent
# - the environment is deployed as the `dev` environment for LunchBadger and prefixed accordingly through the Helm charts and the lunchbager-ui config files as well
# - the local installation assumes you are installing to this domain which must be resolved on your host through dnsmasq or equivalent, many sub-domains are used like `api`, `kube-watch` etc

# this creates the `customer` namespace where LunchBadger user microservices are deployed
customerNamespace:
  enabled: true
actualizer:
  enabled: true
  adminOrigin: ""
  # the user/customer environments are always deployed as their `dev` environments and subdomain prefixed accordingly as well
  customerDomain: dev.lunchbadger.local
  images:
    actualizer:
      tag: 0.2.50
  rbac:
    enabled: true
    pullSecret: awsecr-cred
  serverlessAPIURLTemplate: http://sls-api-$PRODUCER-$ENV.customer
  versions:
    gateway: master
    serverless: validation
    workspace: "0.2.6"
  workspaceAPIURLTemplate: http://workspace-$PRODUCER-$ENV.customer:81/api
configstore:
  enabled: true
  image:
    tag: external_push
  ingressHost: dev-api.lunchbadger.local
gitea:
  enabled: true
  service:
    http:
      serviceType: ClusterIP
      port: 3000
      # sometimes it is necesary to access through an external port i.e. http(s)://<dns-name>:<external-port>, if so switch serviceType to NodePort and assign a NodePort below
      #nodePort: 30280
      externalPort: 3000
      externalHost: localhost
    ssh:
      serviceType: NodePort
      port: 22
      # note this NodePort, this port is externalized to git clone repos
      nodePort: 30222
      targetPort: ssh
      externalHost: localhost
      # when deploying to Minikube - make this your Minikube VM's IP address - `minikube ip`
      # depending on the VM driver, start and stopping / creating and deleting your cluster may change your IP
      # on MacOSX - `rm ~/Library/VirtualBox/HostInterfaceNetworking-vboxnet0-Dhcpd.leases`
      # before re-creating/re-starting your cluster  should reset the last digit of the IP
      # address back to the lowest number, i.e. 100
      externalIPs:
      - 192.168.99.100
  persistence:
    enabled: true
    giteaSize: 40Gi
    postgresSize: 5Gi
    storageClass: standard
    accessMode: ReadWriteOnce
    annotations:
      "helm.sh/resource-policy": keep
 #  existingGiteaClaim: lb-gitea # to reattach to pvc
 #  existingPostgresClaim: lb-postgres # to reattach to pvc
  config:
    # automatically perform Gitea installation
    disableInstaller: true
kubeless:
  enabled: true
# using the Express Gateway as the main point of ingress requires that you assign the external IP address manually (minikube ip) if running in minikube or setting up a bare metal
# loadbalancer like MetalLb, or opening up a NodePort because the gateway service is of type LoadBalancer and usually run in AWS
gateway:
  enabled: true
  global: {}
  image:
    pullPolicy: Always
    repository: expressgateway/express-gateway
    tag: v1.9.1
  jwtPublicKey: |
    -----BEGIN PUBLIC KEY-----
    MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA7ATrA/pvXdzabRID6pBA
    A+i2zez6FG3SXw5peAV2oQUmd64JbO2vUMih4PIt5D/o6gHfzQDwI/5e8wpiNKKp
    81dpvy3uYecyfGT4x+FYQ4xj0p7dnczPlp5t1ottCXYQyyB07UZ4UsOT063CRhgi
    00HhlURBm+yjLwnlZv/VGDNzXNFX1+t+PbGC5Ab7R02Fsnp8TGfjUgzA6NDgerKi
    Jcq/fSxRb5WSB/gscCGGWjvgIJrHOLI9ofaXFpoHCxePCsVkaR0JNz8Q89tIWvWv
    5msm062aD7y1ThfP6I3HeGf3dT6IavLOVD6Wk82/WN+aaQ7BKOstglWqzjJvcEvU
    OQIDAQAB
    -----END PUBLIC KEY-----
  origin: http:/lunchbadger.local
  # IMPORTANT: make this your Minikube VM IP address - `minikube ip`
  externalIPs:
  - 192.168.99.100
  producersAPIKey: {}
  production: false
  # the ingressAddress is the backend endpoint defined by Express Gateawy to route to all Kubernetes services
  # it assumes Traefik is used as the ingress controller and installed as the `traefik` service in the `kube-system` namespace
  ingressAddress: http://traefik.kube-system
grafana:
  enabled: false
kube-watcher:
  enabled: true
  images:
    kubeWatcher:
      pullPolicy: Always
      repository: 410240865662.dkr.ecr.us-west-2.amazonaws.com/kube-watcher
      tag: v2.0.2
  ingressHost: dev-kube-watcher.lunchbadger.local
  rbac:
    enabled: true
    pullSecret: awsecr-cred
kubernetes-dashboard:
  enabled: false
prometheus:
  enabled: false
# check values here: https://github.com/helm/charts/tree/master/stable/redis
redis:
  enabled: true
  master:
    persistence:
      storageClass: standard
graphql:
  enabled: true
storageClass: # enables AWS storage class for running LB on AWS for us-west-2a, us-west-2b availability zones
  enabled: false
storageClass2: # eanbles AWS storage class from running LB on AWS for us-west-2c availability zones
  enabled: false
storageClassCeph:
  enabled: false
  name: standard
# ignore below, traefik values are in a separate values file
# traefik installation not working as dependencey because it needs to be installed in `kube-system` namespace not the `default` namespace
# child chart installation to different namespaces is not supported in Helm 2.x
#traefik:
#  enabled: true
#  imageTag: alpine
#  gzip:
#    enabled: false
#  serviceType: NodePort
#  rbac:
#    enabled: true
#  dashboard:
#    enabled: true
#    domain: traefik.lunchbadger.local
git-api:
  enabled: true
  pullSecret: awsecr-cred
  giteaSecret: eefa9049f13b49b1e120d8b6a9e8b88917e60122
mainIngress:
  enabled: true
  ingressHost: dev-api.lunchbadger.local
